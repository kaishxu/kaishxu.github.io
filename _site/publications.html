<!DOCTYPE html>
<html lang="en">
<!-- Template: https://github.com/luost26/academic-homepage -->
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <title>Publications - > Kaishuai Xu</title>
    
    <!-- Favicon -->
    <link rel="icon" type="image/png" href="/academic-homepage/assets/images/favicon-16x16.png?v=M44lzPylqQ" sizes="16x16">
    <link rel="icon" type="image/png" href="/academic-homepage/assets/images/favicon-32x32.png?v=M44lzPylqQ" sizes="32x32">
    <link rel="icon" type="image/png" href="/academic-homepage/assets/images/favicon-96x96.png?v=M44lzPylqQ" sizes="96x96">
    <!-- <link rel="shortcut icon" href="/academic-homepage/assets/images/favicon.ico?v=M44lzPylqQ"> -->

    <!-- Stylesheets -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.6.0/css/bootstrap.min.css" integrity="sha512-P5MgMn1jBN01asBgU0z60Qk4QxiXo86+wlFahKrsQf37c9cro517WzVSPPV1tDKzhku2iJ2FVgL67wG03SGnNA==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css" integrity="sha512-DTOQO9RWCH3ppGqcWaEA1BIZOC6xxalwEsw9c2QQeAIftl+Vegovlnee1c9QX4TctnWMn13TZye+giMm8e2LwA==" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.1/css/academicons.min.css" integrity="sha512-b1ASx0WHgVFL5ZQhTgiPWX+68KjS38Jk87jg7pe+qC7q9YkEtFq0z7xCglv7qGIs/68d3mAp+StfC8WKC5SSAg==" crossorigin="anonymous" />
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Lato:ital,wght@0,300;0,400;0,700;0,900;1,300;1,400;1,700;1,900&family=Raleway:ital,wght@0,300;0,400;0,500;1,300;1,400;1,500&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css" integrity="sha384-nB0miv6/jRmo5UMMR1wu3Gz6NLsoTkbqJghGIsx//Rlm+ZU03BU6SQNC66uf4l5+" crossorigin="anonymous">
    <link rel="stylesheet" href="/academic-homepage/assets/css/global.css">
</head>
<body class="bg-light" data-spy="scroll" data-target="#navbar-year" data-offset="100">
    <nav class="navbar navbar-expand-sm navbar-light bg-light fixed-top mb-5 shadow-sm">
    <div class="container-lg">
        <a class="navbar-brand"><strong class="blinking-cursor">> Kaishuai Xu</strong></a>
        <button class="navbar-toggler" style="font-size: 1em; padding: 0.5em;" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
            <i class="fas fa-map"></i> Menu
        </button>

        <div class="collapse navbar-collapse" id="navbarResponsive">
            <ul class="navbar-nav ml-auto">
                
                <li class="nav-item ">
                    <a class="nav-link" href="/academic-homepage/">Home</a>
                </li>
                
                <li class="nav-item active">
                    <a class="nav-link" href="/academic-homepage/publications">Publications</a>
                </li>
                
            </ul>
        </div>
    </div>
</nav>

    <div class="container-lg">
        

<div class="row">
    <div class="col-12 col-lg-10">
        
        
        <h2 class="pt-4" id="year-2025">2025</h2>
        <div class="my-0 p-0 bg-white shadow-sm rounded-xl">
            
                
<div class="d-none d-md-block">
    <div class="row no-gutters border-bottom border-gray">
        <div class="col-md-3 col-xl-2 mb-md-0 p-md-3"><img data-src="/academic-homepage/assets/images/covers/rar2.jpg" alt="RAR^2: Retrieval-Augmented Medical Reasoning via Thought-Driven Retrieval" class="lazy w-100 rounded-sm" src="/academic-homepage/assets/images/empty_300x200.png"></div>
        <div class="col-md-9 col-xl-10 p-3 pl-md-0">
            <h5 class="mt-0 mb-1 font-weight-normal">RAR^2: Retrieval-Augmented Medical Reasoning via Thought-Driven Retrieval</h5>
            <p class="mt-0 mb-0 small"><span class="text-body">
            <strong>Kaishuai Xu</strong>, </span><a class="text-body" target="_blank" href="https://wjhou.github.io/">Wenjun Hou*</a>, <a class="text-body" target="_blank" href="https://yicheng98.github.io/">Yi Cheng*</a>, <a class="text-body" target="_blank" href="https://www4.comp.polyu.edu.hk/~cswjli/">Wenjie Li</a>
<mark>(* <i> equal contribution</i>)</mark></p>
            <p class="mt-0 mb-0 small"><i>Findings of the Association for Computational Linguistics: EMNLP 2025</i>   <span class="badge badge-pill badge-publication badge-warning">Findings</span> <span data-semantic-scholar-id=""></span></p>
            <p class="mt-0 mb-0 small text-muted">In this work, we propose RAR^2, a joint learning framework that improves both Reasoning-Augmented Retrieval and Retrieval-Augmented Reasoning. Moreover, we design two test-time scaling strategies to explore the boundaries of our framework.</p>
            
            <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                
            </p>

        </div>
    </div>
</div>

<div class="row no-gutters d-md-none border-bottom border-gray rounded-xl-top  lazy" data-src="/academic-homepage/assets/images/covers/rar2.jpg">
    <div class="w-100 rounded-xl-top " style="background-color: rgba(255,255,255,0.9);">
        <div class="d-flex align-items-start flex-column py-3 px-4">
            <div class="mb-auto"></div>
            <div>
                <h5 class="mt-0 mb-1 font-weight-normal">RAR^2: Retrieval-Augmented Medical Reasoning via Thought-Driven Retrieval</h5>
                <p class="mt-0 mb-0 small"><span class="text-body">
            <strong>Kaishuai Xu</strong>, </span><a class="text-body" target="_blank" href="https://wjhou.github.io/">Wenjun Hou*</a>, <a class="text-body" target="_blank" href="https://yicheng98.github.io/">Yi Cheng*</a>, <a class="text-body" target="_blank" href="https://www4.comp.polyu.edu.hk/~cswjli/">Wenjie Li</a>
<mark>(* <i> equal contribution</i>)</mark></p>
                <p class="mt-0 mb-0 small"><i>Findings of the Association for Computational Linguistics: EMNLP 2025</i>   <span class="badge badge-pill badge-publication badge-warning">Findings</span> <span data-semantic-scholar-id=""></span></p>
                <p class="mt-0 mb-0 small text-muted">In this work, we propose RAR^2, a joint learning framework that improves both Reasoning-Augmented Retrieval and Retrieval-Augmented Reasoning. Moreover, we design two test-time scaling strategies to explore the boundaries of our framework.</p>
                
                <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                    
                </p>
            </div>
        </div>
    </div>

</div>
            
                
<div class="d-none d-md-block">
    <div class="row no-gutters border-bottom border-gray">
        <div class="col-md-3 col-xl-2 mb-md-0 p-md-3"><img data-src="/academic-homepage/assets/images/covers/rise.jpg" alt="Subtle Errors in Reasoning: Preference Learning via Error-injected Self-editing" class="lazy w-100 rounded-sm" src="/academic-homepage/assets/images/empty_300x200.png"></div>
        <div class="col-md-9 col-xl-10 p-3 pl-md-0">
            <h5 class="mt-0 mb-1 font-weight-normal">Subtle Errors in Reasoning: Preference Learning via Error-injected Self-editing</h5>
            <p class="mt-0 mb-0 small"><span class="text-body">
            <strong>Kaishuai Xu</strong>, </span><a class="text-body" target="_blank" href="https://scholar.google.com.hk/citations?user=JK7nNekAAAAJ">Tiezheng Yu</a>, <a class="text-body" target="_blank" href="https://wjhou.github.io/">Wenjun Hou</a>, <a class="text-body" target="_blank" href="https://yicheng98.github.io/">Yi Cheng</a>, <a class="text-body" target="_blank" href="https://cooperleong00.github.io/">Chak Tou Leong</a>, <span class="text-body">
            Liangyou Li, </span><span class="text-body">
            Xin Jiang, </span><span class="text-body">
            Lifeng Shang, </span><a class="text-body" target="_blank" href="https://liuquncn.github.io/index_en.html">Qun Liu</a>, <a class="text-body" target="_blank" href="https://www4.comp.polyu.edu.hk/~cswjli/">Wenjie Li</a></p>
            <p class="mt-0 mb-0 small"><i>Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (ACL)</i> 2025  <span class="badge badge-pill badge-publication badge-primary">Main Conference</span> <span data-semantic-scholar-id=""></span></p>
            <p class="mt-0 mb-0 small text-muted">In this work, we propose a novel preference learning framework called eRror-Injected Self-Editing (RISE), which injects predefined subtle errors into pivotal tokens in reasoning or computation steps to construct hard pairs for error mitigation. Compared with other preference learning methods, RISE further refines the training objective without requiring fine-grained sampling or preference annotation.</p>
            
            <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                
                
                <a target="_blank" href="https://aclanthology.org/2025.acl-long.1506/">[Paper]</a>
                
                
                
                <a target="_blank" href="https://github.com/kaishxu/RISE">[Code]</a>
                
                
            </p>

        </div>
    </div>
</div>

<div class="row no-gutters d-md-none border-bottom border-gray   lazy" data-src="/academic-homepage/assets/images/covers/rise.jpg">
    <div class="w-100  " style="background-color: rgba(255,255,255,0.9);">
        <div class="d-flex align-items-start flex-column py-3 px-4">
            <div class="mb-auto"></div>
            <div>
                <h5 class="mt-0 mb-1 font-weight-normal">Subtle Errors in Reasoning: Preference Learning via Error-injected Self-editing</h5>
                <p class="mt-0 mb-0 small"><span class="text-body">
            <strong>Kaishuai Xu</strong>, </span><a class="text-body" target="_blank" href="https://scholar.google.com.hk/citations?user=JK7nNekAAAAJ">Tiezheng Yu</a>, <a class="text-body" target="_blank" href="https://wjhou.github.io/">Wenjun Hou</a>, <a class="text-body" target="_blank" href="https://yicheng98.github.io/">Yi Cheng</a>, <a class="text-body" target="_blank" href="https://cooperleong00.github.io/">Chak Tou Leong</a>, <span class="text-body">
            Liangyou Li, </span><span class="text-body">
            Xin Jiang, </span><span class="text-body">
            Lifeng Shang, </span><a class="text-body" target="_blank" href="https://liuquncn.github.io/index_en.html">Qun Liu</a>, <a class="text-body" target="_blank" href="https://www4.comp.polyu.edu.hk/~cswjli/">Wenjie Li</a></p>
                <p class="mt-0 mb-0 small"><i>Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (ACL)</i> 2025  <span class="badge badge-pill badge-publication badge-primary">Main Conference</span> <span data-semantic-scholar-id=""></span></p>
                <p class="mt-0 mb-0 small text-muted">In this work, we propose a novel preference learning framework called eRror-Injected Self-Editing (RISE), which injects predefined subtle errors into pivotal tokens in reasoning or computation steps to construct hard pairs for error mitigation. Compared with other preference learning methods, RISE further refines the training objective without requiring fine-grained sampling or preference annotation.</p>
                
                <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                    
                    
                    <a target="_blank" href="https://aclanthology.org/2025.acl-long.1506/">[Paper]</a>
                    
                    
                    
                    <a target="_blank" href="https://github.com/kaishxu/RISE">[Code]</a>
                    
                    
                </p>
            </div>
        </div>
    </div>

</div>
            
                
<div class="d-none d-md-block">
    <div class="row no-gutters border-bottom border-gray">
        <div class="col-md-3 col-xl-2 mb-md-0 p-md-3"><img data-src="/academic-homepage/assets/images/covers/radar.jpg" alt="RADAR: Enhancing Radiology Report Generation with Supplementary Knowledge Injection" class="lazy w-100 rounded-sm" src="/academic-homepage/assets/images/empty_300x200.png"></div>
        <div class="col-md-9 col-xl-10 p-3 pl-md-0">
            <h5 class="mt-0 mb-1 font-weight-normal">RADAR: Enhancing Radiology Report Generation with Supplementary Knowledge Injection</h5>
            <p class="mt-0 mb-0 small"><a class="text-body" target="_blank" href="https://wjhou.github.io/">Wenjun Hou</a>, <a class="text-body" target="_blank" href="https://yicheng98.github.io/">Yi Cheng*</a>, <span class="text-body">
            <strong>Kaishuai Xu*</strong>, </span><span class="text-body">
            Heng Li, </span><span class="text-body">
            Yan Hu, </span><a class="text-body" target="_blank" href="https://www4.comp.polyu.edu.hk/~cswjli/">Wenjie Li</a>, <a class="text-body" target="_blank" href="https://www.sustech.edu.cn/en/faculties/liujiang.html">Jiang Liu</a>
<mark>(* <i> equal contribution</i>)</mark></p>
            <p class="mt-0 mb-0 small"><i>Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (ACL)</i> 2025  <span class="badge badge-pill badge-publication badge-primary">Main Conference</span> <span data-semantic-scholar-id=""></span></p>
            <p class="mt-0 mb-0 small text-muted">We propose Radar, a framework for enhancing radiology report generation with supplementary knowledge injection. Radar improves report generation by systematically leveraging both the internal knowledge of an LLM and externally retrieved information. </p>
            
            <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                
                
                <a target="_blank" href="https://aclanthology.org/2025.acl-long.1279/">[Paper]</a>
                
                
                
                <a target="_blank" href="https://github.com/wjhou/Radar">[Code]</a>
                
                
            </p>

        </div>
    </div>
</div>

<div class="row no-gutters d-md-none border-bottom border-gray   lazy" data-src="/academic-homepage/assets/images/covers/radar.jpg">
    <div class="w-100  " style="background-color: rgba(255,255,255,0.9);">
        <div class="d-flex align-items-start flex-column py-3 px-4">
            <div class="mb-auto"></div>
            <div>
                <h5 class="mt-0 mb-1 font-weight-normal">RADAR: Enhancing Radiology Report Generation with Supplementary Knowledge Injection</h5>
                <p class="mt-0 mb-0 small"><a class="text-body" target="_blank" href="https://wjhou.github.io/">Wenjun Hou</a>, <a class="text-body" target="_blank" href="https://yicheng98.github.io/">Yi Cheng*</a>, <span class="text-body">
            <strong>Kaishuai Xu*</strong>, </span><span class="text-body">
            Heng Li, </span><span class="text-body">
            Yan Hu, </span><a class="text-body" target="_blank" href="https://www4.comp.polyu.edu.hk/~cswjli/">Wenjie Li</a>, <a class="text-body" target="_blank" href="https://www.sustech.edu.cn/en/faculties/liujiang.html">Jiang Liu</a>
<mark>(* <i> equal contribution</i>)</mark></p>
                <p class="mt-0 mb-0 small"><i>Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (ACL)</i> 2025  <span class="badge badge-pill badge-publication badge-primary">Main Conference</span> <span data-semantic-scholar-id=""></span></p>
                <p class="mt-0 mb-0 small text-muted">We propose Radar, a framework for enhancing radiology report generation with supplementary knowledge injection. Radar improves report generation by systematically leveraging both the internal knowledge of an LLM and externally retrieved information. </p>
                
                <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                    
                    
                    <a target="_blank" href="https://aclanthology.org/2025.acl-long.1279/">[Paper]</a>
                    
                    
                    
                    <a target="_blank" href="https://github.com/wjhou/Radar">[Code]</a>
                    
                    
                </p>
            </div>
        </div>
    </div>

</div>
            
                
<div class="d-none d-md-block">
    <div class="row no-gutters border-bottom border-gray">
        <div class="col-md-3 col-xl-2 mb-md-0 p-md-3"><img data-src="/academic-homepage/assets/images/covers/arjudge.jpg" alt="Learning to Align Multi-Faceted Evaluation: A Unified and Robust Framework" class="lazy w-100 rounded-sm" src="/academic-homepage/assets/images/empty_300x200.png"></div>
        <div class="col-md-9 col-xl-10 p-3 pl-md-0">
            <h5 class="mt-0 mb-1 font-weight-normal">Learning to Align Multi-Faceted Evaluation: A Unified and Robust Framework</h5>
            <p class="mt-0 mb-0 small"><span class="text-body">
            <strong>Kaishuai Xu</strong>, </span><a class="text-body" target="_blank" href="https://scholar.google.com.hk/citations?user=JK7nNekAAAAJ">Tiezheng Yu</a>, <a class="text-body" target="_blank" href="https://yicheng98.github.io/">Yi Cheng</a>, <a class="text-body" target="_blank" href="https://wjhou.github.io/">Wenjun Hou</a>, <span class="text-body">
            Liangyou Li, </span><span class="text-body">
            Xin Jiang, </span><span class="text-body">
            Lifeng Shang, </span><a class="text-body" target="_blank" href="https://liuquncn.github.io/index_en.html">Qun Liu</a>, <a class="text-body" target="_blank" href="https://www4.comp.polyu.edu.hk/~cswjli/">Wenjie Li</a></p>
            <p class="mt-0 mb-0 small"><i>Findings of the Association for Computational Linguistics: ACL 2025</i>   <span class="badge badge-pill badge-publication badge-warning">Findings</span> <span data-semantic-scholar-id=""></span></p>
            <p class="mt-0 mb-0 small text-muted">We propose a novel evaluation framework, ARJudge, that adaptively formulates evaluation criteria and synthesizes both text-based and code-driven analyses to evaluate LLM responses.</p>
            
            <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                
                
                <a target="_blank" href="https://aclanthology.org/2025.findings-acl.494/">[Paper]</a>
                
                
                
                <a target="_blank" href="https://github.com/kaishxu/ARJudge">[Code]</a>
                
                
            </p>

        </div>
    </div>
</div>

<div class="row no-gutters d-md-none border-bottom border-gray   lazy" data-src="/academic-homepage/assets/images/covers/arjudge.jpg">
    <div class="w-100  " style="background-color: rgba(255,255,255,0.9);">
        <div class="d-flex align-items-start flex-column py-3 px-4">
            <div class="mb-auto"></div>
            <div>
                <h5 class="mt-0 mb-1 font-weight-normal">Learning to Align Multi-Faceted Evaluation: A Unified and Robust Framework</h5>
                <p class="mt-0 mb-0 small"><span class="text-body">
            <strong>Kaishuai Xu</strong>, </span><a class="text-body" target="_blank" href="https://scholar.google.com.hk/citations?user=JK7nNekAAAAJ">Tiezheng Yu</a>, <a class="text-body" target="_blank" href="https://yicheng98.github.io/">Yi Cheng</a>, <a class="text-body" target="_blank" href="https://wjhou.github.io/">Wenjun Hou</a>, <span class="text-body">
            Liangyou Li, </span><span class="text-body">
            Xin Jiang, </span><span class="text-body">
            Lifeng Shang, </span><a class="text-body" target="_blank" href="https://liuquncn.github.io/index_en.html">Qun Liu</a>, <a class="text-body" target="_blank" href="https://www4.comp.polyu.edu.hk/~cswjli/">Wenjie Li</a></p>
                <p class="mt-0 mb-0 small"><i>Findings of the Association for Computational Linguistics: ACL 2025</i>   <span class="badge badge-pill badge-publication badge-warning">Findings</span> <span data-semantic-scholar-id=""></span></p>
                <p class="mt-0 mb-0 small text-muted">We propose a novel evaluation framework, ARJudge, that adaptively formulates evaluation criteria and synthesizes both text-based and code-driven analyses to evaluate LLM responses.</p>
                
                <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                    
                    
                    <a target="_blank" href="https://aclanthology.org/2025.findings-acl.494/">[Paper]</a>
                    
                    
                    
                    <a target="_blank" href="https://github.com/kaishxu/ARJudge">[Code]</a>
                    
                    
                </p>
            </div>
        </div>
    </div>

</div>
            
                
<div class="d-none d-md-block">
    <div class="row no-gutters border-bottom border-gray">
        <div class="col-md-3 col-xl-2 mb-md-0 p-md-3"><img data-src="/academic-homepage/assets/images/covers/llm-diagnosis.jpg" alt="Large language models for disease diagnosis: a scoping review" class="lazy w-100 rounded-sm" src="/academic-homepage/assets/images/empty_300x200.png"></div>
        <div class="col-md-9 col-xl-10 p-3 pl-md-0">
            <h5 class="mt-0 mb-1 font-weight-normal">Large language models for disease diagnosis: a scoping review</h5>
            <p class="mt-0 mb-0 small"><span class="text-body">
            Shuang Zhou<sup>#</sup>, </span><span class="text-body">
            Zidu Xu, </span><span class="text-body">
            Mian Zhang, </span><span class="text-body">
            Chunpu Xu, </span><span class="text-body">
            Yawen Guo, </span><span class="text-body">
            Zaifu Zhan, </span><span class="text-body">
            Yi Fang, </span><span class="text-body">
            Sirui Ding, </span><span class="text-body">
            Jiashuo Wang, </span><span class="text-body">
            <strong>Kaishuai Xu</strong>, </span><span class="text-body">
            Liqiao Xia, </span><span class="text-body">
            Jeremy Yeung, </span><span class="text-body">
            Daochen Zha, </span><span class="text-body">
            Dongming Cai, </span><span class="text-body">
            Genevieve B. Melton, </span><span class="text-body">
            Mingquan Lin, </span><span class="text-body">
            Rui Zhang</span>
<mark>(<sup>#</sup> <i> corresponding author</i>)</mark></p>
            <p class="mt-0 mb-0 small"><i>npj Artificial Intelligence 1, Article number: 9</i> (2025)  <span data-semantic-scholar-id=""></span></p>
            <p class="mt-0 mb-0 small text-muted">In this article, we perform a comprehensive review of LLM-based methods for disease diagnosis. Our review examines the existing literature across various dimensions, including disease types and associated clinical specialties, clinical data, LLM techniques, and evaluation methods.</p>
            
            <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                
                
                <a target="_blank" href="https://www.nature.com/articles/s44387-025-00011-z">[Paper]</a>
                
                
            </p>

        </div>
    </div>
</div>

<div class="row no-gutters d-md-none border-bottom border-gray   lazy" data-src="/academic-homepage/assets/images/covers/llm-diagnosis.jpg">
    <div class="w-100  " style="background-color: rgba(255,255,255,0.9);">
        <div class="d-flex align-items-start flex-column py-3 px-4">
            <div class="mb-auto"></div>
            <div>
                <h5 class="mt-0 mb-1 font-weight-normal">Large language models for disease diagnosis: a scoping review</h5>
                <p class="mt-0 mb-0 small"><span class="text-body">
            Shuang Zhou<sup>#</sup>, </span><span class="text-body">
            Zidu Xu, </span><span class="text-body">
            Mian Zhang, </span><span class="text-body">
            Chunpu Xu, </span><span class="text-body">
            Yawen Guo, </span><span class="text-body">
            Zaifu Zhan, </span><span class="text-body">
            Yi Fang, </span><span class="text-body">
            Sirui Ding, </span><span class="text-body">
            Jiashuo Wang, </span><span class="text-body">
            <strong>Kaishuai Xu</strong>, </span><span class="text-body">
            Liqiao Xia, </span><span class="text-body">
            Jeremy Yeung, </span><span class="text-body">
            Daochen Zha, </span><span class="text-body">
            Dongming Cai, </span><span class="text-body">
            Genevieve B. Melton, </span><span class="text-body">
            Mingquan Lin, </span><span class="text-body">
            Rui Zhang</span>
<mark>(<sup>#</sup> <i> corresponding author</i>)</mark></p>
                <p class="mt-0 mb-0 small"><i>npj Artificial Intelligence 1, Article number: 9</i> (2025)  <span data-semantic-scholar-id=""></span></p>
                <p class="mt-0 mb-0 small text-muted">In this article, we perform a comprehensive review of LLM-based methods for disease diagnosis. Our review examines the existing literature across various dimensions, including disease types and associated clinical specialties, clinical data, LLM techniques, and evaluation methods.</p>
                
                <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                    
                    
                    <a target="_blank" href="https://www.nature.com/articles/s44387-025-00011-z">[Paper]</a>
                    
                    
                </p>
            </div>
        </div>
    </div>

</div>
            
                
<div class="d-none d-md-block">
    <div class="row no-gutters  border-gray">
        <div class="col-md-3 col-xl-2 mb-md-0 p-md-3"><img data-src="/academic-homepage/assets/images/covers/integrative-decoding.jpg" alt="Integrative Decoding: Improving Factuality via Implicit Self-consistency" class="lazy w-100 rounded-sm" src="/academic-homepage/assets/images/empty_300x200.png"></div>
        <div class="col-md-9 col-xl-10 p-3 pl-md-0">
            <h5 class="mt-0 mb-1 font-weight-normal">Integrative Decoding: Improving Factuality via Implicit Self-consistency</h5>
            <p class="mt-0 mb-0 small"><a class="text-body" target="_blank" href="https://yicheng98.github.io/">Yi Cheng</a>, <span class="text-body">
            Xiao Liang, </span><span class="text-body">
            Yeyun Gong, </span><span class="text-body">
            Wen Xiao, </span><span class="text-body">
            Song Wang, </span><span class="text-body">
            Yuji Zhang, </span><a class="text-body" target="_blank" href="https://wjhou.github.io/">Wenjun Hou</a>, <span class="text-body">
            <strong>Kaishuai Xu</strong>, </span><span class="text-body">
            Wenge Liu, </span><a class="text-body" target="_blank" href="https://www4.comp.polyu.edu.hk/~cswjli/">Wenjie Li</a>, <span class="text-body">
            Jian Jiao, </span><span class="text-body">
            Qi Chen, </span><span class="text-body">
            Peng Cheng, </span><span class="text-body">
            Wayne Xiong</span></p>
            <p class="mt-0 mb-0 small"><i>The Thirteenth International Conference on Learning Representations (ICLR 2025)</i>   <span class="badge badge-pill badge-publication badge-danger">Poster</span> <span data-semantic-scholar-id=""></span></p>
            <p class="mt-0 mb-0 small text-muted">In this paper, we present Integrative Decoding (ID), to unlock the potential of self-consistency in open-ended generation tasks. ID operates by constructing a set of inputs, each prepended with a previously sampled response, and then processes them concurrently, with the next token being selected by aggregating of all their corresponding predictions at each decoding step. In essence, this simple approach implicitly incorporates self-consistency in the decoding objective.</p>
            
            <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                
                
                <a target="_blank" href="https://openreview.net/forum?id=gGWYecsK1U">[Paper]</a>
                
                
                
                <a target="_blank" href="https://github.com/YiCheng98/IntegrativeDecoding">[Code]</a>
                
                
            </p>

        </div>
    </div>
</div>

<div class="row no-gutters d-md-none  border-gray  rounded-xl-bottom lazy" data-src="/academic-homepage/assets/images/covers/integrative-decoding.jpg">
    <div class="w-100  rounded-xl-bottom" style="background-color: rgba(255,255,255,0.9);">
        <div class="d-flex align-items-start flex-column py-3 px-4">
            <div class="mb-auto"></div>
            <div>
                <h5 class="mt-0 mb-1 font-weight-normal">Integrative Decoding: Improving Factuality via Implicit Self-consistency</h5>
                <p class="mt-0 mb-0 small"><a class="text-body" target="_blank" href="https://yicheng98.github.io/">Yi Cheng</a>, <span class="text-body">
            Xiao Liang, </span><span class="text-body">
            Yeyun Gong, </span><span class="text-body">
            Wen Xiao, </span><span class="text-body">
            Song Wang, </span><span class="text-body">
            Yuji Zhang, </span><a class="text-body" target="_blank" href="https://wjhou.github.io/">Wenjun Hou</a>, <span class="text-body">
            <strong>Kaishuai Xu</strong>, </span><span class="text-body">
            Wenge Liu, </span><a class="text-body" target="_blank" href="https://www4.comp.polyu.edu.hk/~cswjli/">Wenjie Li</a>, <span class="text-body">
            Jian Jiao, </span><span class="text-body">
            Qi Chen, </span><span class="text-body">
            Peng Cheng, </span><span class="text-body">
            Wayne Xiong</span></p>
                <p class="mt-0 mb-0 small"><i>The Thirteenth International Conference on Learning Representations (ICLR 2025)</i>   <span class="badge badge-pill badge-publication badge-danger">Poster</span> <span data-semantic-scholar-id=""></span></p>
                <p class="mt-0 mb-0 small text-muted">In this paper, we present Integrative Decoding (ID), to unlock the potential of self-consistency in open-ended generation tasks. ID operates by constructing a set of inputs, each prepended with a previously sampled response, and then processes them concurrently, with the next token being selected by aggregating of all their corresponding predictions at each decoding step. In essence, this simple approach implicitly incorporates self-consistency in the decoding objective.</p>
                
                <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                    
                    
                    <a target="_blank" href="https://openreview.net/forum?id=gGWYecsK1U">[Paper]</a>
                    
                    
                    
                    <a target="_blank" href="https://github.com/YiCheng98/IntegrativeDecoding">[Code]</a>
                    
                    
                </p>
            </div>
        </div>
    </div>

</div>
            
        </div>
        
        
        <h2 class="pt-4" id="year-2024">2024</h2>
        <div class="my-0 p-0 bg-white shadow-sm rounded-xl">
            
                
<div class="d-none d-md-block">
    <div class="row no-gutters border-bottom border-gray">
        <div class="col-md-3 col-xl-2 mb-md-0 p-md-3"><img data-src="/academic-homepage/assets/images/covers/scan.jpg" alt="Memory-Augmented Multimodal LLMs for Surgical VQA via Self-Contained Inquiry" class="lazy w-100 rounded-sm" src="/academic-homepage/assets/images/empty_300x200.png"></div>
        <div class="col-md-9 col-xl-10 p-3 pl-md-0">
            <h5 class="mt-0 mb-1 font-weight-normal">Memory-Augmented Multimodal LLMs for Surgical VQA via Self-Contained Inquiry</h5>
            <p class="mt-0 mb-0 small"><a class="text-body" target="_blank" href="https://wjhou.github.io/">Wenjun Hou</a>, <a class="text-body" target="_blank" href="https://yicheng98.github.io/">Yi Cheng*</a>, <span class="text-body">
            <strong>Kaishuai Xu*</strong>, </span><span class="text-body">
            Yan Hu, </span><a class="text-body" target="_blank" href="https://www4.comp.polyu.edu.hk/~cswjli/">Wenjie Li</a>, <a class="text-body" target="_blank" href="https://www.sustech.edu.cn/en/faculties/liujiang.html">Jiang Liu</a>
<mark>(* <i> equal contribution</i>)</mark></p>
            <p class="mt-0 mb-0 small"><i>arXiv preprint</i> 2024  <span class="badge badge-pill badge-publication badge-secondary">Preprint</span> <span data-semantic-scholar-id=""></span></p>
            <p class="mt-0 mb-0 small text-muted">We propose SCAN, a simple yet effective memory-augmented framework that leverages Multimodal LLMs to improve surgical context comprehension via Self-Contained Inquiry.</p>
            
            <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                
                
                <a target="_blank" href="https://arxiv.org/abs/2411.10937">[Paper]</a>
                
                
            </p>

        </div>
    </div>
</div>

<div class="row no-gutters d-md-none border-bottom border-gray rounded-xl-top  lazy" data-src="/academic-homepage/assets/images/covers/scan.jpg">
    <div class="w-100 rounded-xl-top " style="background-color: rgba(255,255,255,0.9);">
        <div class="d-flex align-items-start flex-column py-3 px-4">
            <div class="mb-auto"></div>
            <div>
                <h5 class="mt-0 mb-1 font-weight-normal">Memory-Augmented Multimodal LLMs for Surgical VQA via Self-Contained Inquiry</h5>
                <p class="mt-0 mb-0 small"><a class="text-body" target="_blank" href="https://wjhou.github.io/">Wenjun Hou</a>, <a class="text-body" target="_blank" href="https://yicheng98.github.io/">Yi Cheng*</a>, <span class="text-body">
            <strong>Kaishuai Xu*</strong>, </span><span class="text-body">
            Yan Hu, </span><a class="text-body" target="_blank" href="https://www4.comp.polyu.edu.hk/~cswjli/">Wenjie Li</a>, <a class="text-body" target="_blank" href="https://www.sustech.edu.cn/en/faculties/liujiang.html">Jiang Liu</a>
<mark>(* <i> equal contribution</i>)</mark></p>
                <p class="mt-0 mb-0 small"><i>arXiv preprint</i> 2024  <span class="badge badge-pill badge-publication badge-secondary">Preprint</span> <span data-semantic-scholar-id=""></span></p>
                <p class="mt-0 mb-0 small text-muted">We propose SCAN, a simple yet effective memory-augmented framework that leverages Multimodal LLMs to improve surgical context comprehension via Self-Contained Inquiry.</p>
                
                <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                    
                    
                    <a target="_blank" href="https://arxiv.org/abs/2411.10937">[Paper]</a>
                    
                    
                </p>
            </div>
        </div>
    </div>

</div>
            
                
<div class="d-none d-md-block">
    <div class="row no-gutters border-bottom border-gray">
        <div class="col-md-3 col-xl-2 mb-md-0 p-md-3"><img data-src="/academic-homepage/assets/images/covers/acoustic-landmarks.jpg" alt="When LLMs Meets Acoustic Landmarks: An Efficient Approach to Integrate Speech into Large Language Models for Depression Detection" class="lazy w-100 rounded-sm" src="/academic-homepage/assets/images/empty_300x200.png"></div>
        <div class="col-md-9 col-xl-10 p-3 pl-md-0">
            <h5 class="mt-0 mb-1 font-weight-normal">When LLMs Meets Acoustic Landmarks: An Efficient Approach to Integrate Speech into Large Language Models for Depression Detection</h5>
            <p class="mt-0 mb-0 small"><a class="text-body" target="_blank" href="https://sites.google.com/view/xiangyu-zhang/home">Xiangyu Zhang</a>, <span class="text-body">
            Hexin Liu, </span><span class="text-body">
            <strong>Kaishuai Xu</strong>, </span><span class="text-body">
            Qiquan Zhang, </span><span class="text-body">
            Daijiao Liu, </span><span class="text-body">
            Beena Ahmed, </span><a class="text-body" target="_blank" href="http://maestro.ee.unsw.edu.au/~julien/">Julien Epps</a></p>
            <p class="mt-0 mb-0 small"><i>Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing (EMNLP)</i> 2024  <span class="badge badge-pill badge-publication badge-primary">Main Conference</span> <span data-semantic-scholar-id=""></span></p>
            <p class="mt-0 mb-0 small text-muted">We investigate an efficient method for depression detection by integrating speech signals into LLMs utilizing Acoustic Landmarks. By incorporating acoustic landmarks, which are specific to the pronunciation of spoken words, our method adds critical dimensions to text transcripts. This integration also provides insights into the unique speech patterns of individuals, revealing the potential mental states of individuals.</p>
            
            <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                
                
                <a target="_blank" href="https://aclanthology.org/2024.emnlp-main.8/">[Paper]</a>
                
                
            </p>

        </div>
    </div>
</div>

<div class="row no-gutters d-md-none border-bottom border-gray   lazy" data-src="/academic-homepage/assets/images/covers/acoustic-landmarks.jpg">
    <div class="w-100  " style="background-color: rgba(255,255,255,0.9);">
        <div class="d-flex align-items-start flex-column py-3 px-4">
            <div class="mb-auto"></div>
            <div>
                <h5 class="mt-0 mb-1 font-weight-normal">When LLMs Meets Acoustic Landmarks: An Efficient Approach to Integrate Speech into Large Language Models for Depression Detection</h5>
                <p class="mt-0 mb-0 small"><a class="text-body" target="_blank" href="https://sites.google.com/view/xiangyu-zhang/home">Xiangyu Zhang</a>, <span class="text-body">
            Hexin Liu, </span><span class="text-body">
            <strong>Kaishuai Xu</strong>, </span><span class="text-body">
            Qiquan Zhang, </span><span class="text-body">
            Daijiao Liu, </span><span class="text-body">
            Beena Ahmed, </span><a class="text-body" target="_blank" href="http://maestro.ee.unsw.edu.au/~julien/">Julien Epps</a></p>
                <p class="mt-0 mb-0 small"><i>Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing (EMNLP)</i> 2024  <span class="badge badge-pill badge-publication badge-primary">Main Conference</span> <span data-semantic-scholar-id=""></span></p>
                <p class="mt-0 mb-0 small text-muted">We investigate an efficient method for depression detection by integrating speech signals into LLMs utilizing Acoustic Landmarks. By incorporating acoustic landmarks, which are specific to the pronunciation of spoken words, our method adds critical dimensions to text transcripts. This integration also provides insights into the unique speech patterns of individuals, revealing the potential mental states of individuals.</p>
                
                <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                    
                    
                    <a target="_blank" href="https://aclanthology.org/2024.emnlp-main.8/">[Paper]</a>
                    
                    
                </p>
            </div>
        </div>
    </div>

</div>
            
                
<div class="d-none d-md-block">
    <div class="row no-gutters border-bottom border-gray">
        <div class="col-md-3 col-xl-2 mb-md-0 p-md-3"><img data-src="/academic-homepage/assets/images/covers/icon.jpg" alt="ICON: Improving Inter-Report Consistency in Radiology Report Generation via Lesion-aware Mixup Augmentation" class="lazy w-100 rounded-sm" src="/academic-homepage/assets/images/empty_300x200.png"></div>
        <div class="col-md-9 col-xl-10 p-3 pl-md-0">
            <h5 class="mt-0 mb-1 font-weight-normal">ICON: Improving Inter-Report Consistency in Radiology Report Generation via Lesion-aware Mixup Augmentation</h5>
            <p class="mt-0 mb-0 small"><a class="text-body" target="_blank" href="https://wjhou.github.io/">Wenjun Hou</a>, <a class="text-body" target="_blank" href="https://yicheng98.github.io/">Yi Cheng*</a>, <span class="text-body">
            <strong>Kaishuai Xu*</strong>, </span><span class="text-body">
            Yan Hu, </span><a class="text-body" target="_blank" href="https://www4.comp.polyu.edu.hk/~cswjli/">Wenjie Li</a>, <a class="text-body" target="_blank" href="https://www.sustech.edu.cn/en/faculties/liujiang.html">Jiang Liu</a>
<mark>(* <i> equal contribution</i>)</mark></p>
            <p class="mt-0 mb-0 small"><i>Findings of the Association for Computational Linguistics: EMNLP 2024</i>   <span class="badge badge-pill badge-publication badge-warning">Findings</span> <span data-semantic-scholar-id=""></span></p>
            <p class="mt-0 mb-0 small text-muted">We propose ICON, which improves the inter-report consistency of radiology report generation. Aiming to enhance the system’s ability to capture similarities in semantically equivalent lesions, our approach first involves extracting lesions from input images and examining their characteristics. Then, we introduce a lesion-aware mixup technique to ensure that the representations of the semantically equivalent lesions align with the same attributes, achieved through a linear combination during the training phase. Extensive experiments on three publicly available chest X-ray datasets verify the effectiveness of our approach, both in terms of improving the consistency and accuracy of the generated reports.</p>
            
            <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                
                
                <a target="_blank" href="https://aclanthology.org/2024.findings-emnlp.528/">[Paper]</a>
                
                
                
                <a target="_blank" href="https://github.com/wjhou/ICon">[Code]</a>
                
                
            </p>

        </div>
    </div>
</div>

<div class="row no-gutters d-md-none border-bottom border-gray   lazy" data-src="/academic-homepage/assets/images/covers/icon.jpg">
    <div class="w-100  " style="background-color: rgba(255,255,255,0.9);">
        <div class="d-flex align-items-start flex-column py-3 px-4">
            <div class="mb-auto"></div>
            <div>
                <h5 class="mt-0 mb-1 font-weight-normal">ICON: Improving Inter-Report Consistency in Radiology Report Generation via Lesion-aware Mixup Augmentation</h5>
                <p class="mt-0 mb-0 small"><a class="text-body" target="_blank" href="https://wjhou.github.io/">Wenjun Hou</a>, <a class="text-body" target="_blank" href="https://yicheng98.github.io/">Yi Cheng*</a>, <span class="text-body">
            <strong>Kaishuai Xu*</strong>, </span><span class="text-body">
            Yan Hu, </span><a class="text-body" target="_blank" href="https://www4.comp.polyu.edu.hk/~cswjli/">Wenjie Li</a>, <a class="text-body" target="_blank" href="https://www.sustech.edu.cn/en/faculties/liujiang.html">Jiang Liu</a>
<mark>(* <i> equal contribution</i>)</mark></p>
                <p class="mt-0 mb-0 small"><i>Findings of the Association for Computational Linguistics: EMNLP 2024</i>   <span class="badge badge-pill badge-publication badge-warning">Findings</span> <span data-semantic-scholar-id=""></span></p>
                <p class="mt-0 mb-0 small text-muted">We propose ICON, which improves the inter-report consistency of radiology report generation. Aiming to enhance the system’s ability to capture similarities in semantically equivalent lesions, our approach first involves extracting lesions from input images and examining their characteristics. Then, we introduce a lesion-aware mixup technique to ensure that the representations of the semantically equivalent lesions align with the same attributes, achieved through a linear combination during the training phase. Extensive experiments on three publicly available chest X-ray datasets verify the effectiveness of our approach, both in terms of improving the consistency and accuracy of the generated reports.</p>
                
                <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                    
                    
                    <a target="_blank" href="https://aclanthology.org/2024.findings-emnlp.528/">[Paper]</a>
                    
                    
                    
                    <a target="_blank" href="https://github.com/wjhou/ICon">[Code]</a>
                    
                    
                </p>
            </div>
        </div>
    </div>

</div>
            
                
<div class="d-none d-md-block">
    <div class="row no-gutters border-bottom border-gray">
        <div class="col-md-3 col-xl-2 mb-md-0 p-md-3"><img data-src="/academic-homepage/assets/images/covers/emulation.jpg" alt="Reasoning Like a Doctor: Improving Medical Dialogue Systems via Diagnostic Reasoning Process Alignment" class="lazy w-100 rounded-sm" src="/academic-homepage/assets/images/empty_300x200.png"></div>
        <div class="col-md-9 col-xl-10 p-3 pl-md-0">
            <h5 class="mt-0 mb-1 font-weight-normal">Reasoning Like a Doctor: Improving Medical Dialogue Systems via Diagnostic Reasoning Process Alignment</h5>
            <p class="mt-0 mb-0 small"><span class="text-body">
            <strong>Kaishuai Xu</strong>, </span><a class="text-body" target="_blank" href="https://yicheng98.github.io/">Yi Cheng*</a>, <a class="text-body" target="_blank" href="https://wjhou.github.io/">Wenjun Hou*</a>, <a class="text-body" target="_blank" href="https://qiaoyu-tan.github.io/">Qiaoyu Tan</a>, <a class="text-body" target="_blank" href="https://www4.comp.polyu.edu.hk/~cswjli/">Wenjie Li</a>
<mark>(* <i> equal contribution</i>)</mark></p>
            <p class="mt-0 mb-0 small"><i>Findings of the Association for Computational Linguistics: ACL 2024</i>   <span class="badge badge-pill badge-publication badge-warning">Findings</span> <span data-semantic-scholar-id=""></span></p>
            <p class="mt-0 mb-0 small text-muted">We propose a novel framework, Emulation, designed to generate an appropriate response that relies on abductive and deductive diagnostic reasoning analyses and aligns with clinician preferences through thought process modeling. Experimental results on two datasets confirm the efficacy of Emulation. Crucially, our framework furnishes clear explanations for the generated responses, enhancing its transparency in medical consultations.</p>
            
            <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                
                
                <a target="_blank" href="https://aclanthology.org/2024.findings-acl.406/">[Paper]</a>
                
                
                
                <a target="_blank" href="https://github.com/kaishxu/Emulation">[Code]</a>
                
                
            </p>

        </div>
    </div>
</div>

<div class="row no-gutters d-md-none border-bottom border-gray   lazy" data-src="/academic-homepage/assets/images/covers/emulation.jpg">
    <div class="w-100  " style="background-color: rgba(255,255,255,0.9);">
        <div class="d-flex align-items-start flex-column py-3 px-4">
            <div class="mb-auto"></div>
            <div>
                <h5 class="mt-0 mb-1 font-weight-normal">Reasoning Like a Doctor: Improving Medical Dialogue Systems via Diagnostic Reasoning Process Alignment</h5>
                <p class="mt-0 mb-0 small"><span class="text-body">
            <strong>Kaishuai Xu</strong>, </span><a class="text-body" target="_blank" href="https://yicheng98.github.io/">Yi Cheng*</a>, <a class="text-body" target="_blank" href="https://wjhou.github.io/">Wenjun Hou*</a>, <a class="text-body" target="_blank" href="https://qiaoyu-tan.github.io/">Qiaoyu Tan</a>, <a class="text-body" target="_blank" href="https://www4.comp.polyu.edu.hk/~cswjli/">Wenjie Li</a>
<mark>(* <i> equal contribution</i>)</mark></p>
                <p class="mt-0 mb-0 small"><i>Findings of the Association for Computational Linguistics: ACL 2024</i>   <span class="badge badge-pill badge-publication badge-warning">Findings</span> <span data-semantic-scholar-id=""></span></p>
                <p class="mt-0 mb-0 small text-muted">We propose a novel framework, Emulation, designed to generate an appropriate response that relies on abductive and deductive diagnostic reasoning analyses and aligns with clinician preferences through thought process modeling. Experimental results on two datasets confirm the efficacy of Emulation. Crucially, our framework furnishes clear explanations for the generated responses, enhancing its transparency in medical consultations.</p>
                
                <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                    
                    
                    <a target="_blank" href="https://aclanthology.org/2024.findings-acl.406/">[Paper]</a>
                    
                    
                    
                    <a target="_blank" href="https://github.com/kaishxu/Emulation">[Code]</a>
                    
                    
                </p>
            </div>
        </div>
    </div>

</div>
            
                
<div class="d-none d-md-block">
    <div class="row no-gutters border-bottom border-gray">
        <div class="col-md-3 col-xl-2 mb-md-0 p-md-3"><img data-src="/academic-homepage/assets/images/covers/autopal.jpg" alt="AutoPal: Autonomous Adaptation to Users for Personal AI Companionship" class="lazy w-100 rounded-sm" src="/academic-homepage/assets/images/empty_300x200.png"></div>
        <div class="col-md-9 col-xl-10 p-3 pl-md-0">
            <h5 class="mt-0 mb-1 font-weight-normal">AutoPal: Autonomous Adaptation to Users for Personal AI Companionship</h5>
            <p class="mt-0 mb-0 small"><a class="text-body" target="_blank" href="https://yicheng98.github.io/">Yi Cheng</a>, <span class="text-body">
            Wenge Liu, </span><span class="text-body">
            <strong>Kaishuai Xu</strong>, </span><a class="text-body" target="_blank" href="https://wjhou.github.io/">Wenjun Hou</a>, <span class="text-body">
            Yi Ouyang, </span><a class="text-body" target="_blank" href="https://cooperleong00.github.io/">Chak Tou Leong</a>, <a class="text-body" target="_blank" href="https://www4.comp.polyu.edu.hk/~cswjli/">Wenjie Li</a>, <span class="text-body">
            Xian Wu, </span><a class="text-body" target="_blank" href="https://en.westlake.edu.cn/faculty/yefeng-zheng.html">Yefeng Zheng</a></p>
            <p class="mt-0 mb-0 small"><i>arXiv preprint</i> 2024  <span class="badge badge-pill badge-publication badge-secondary">Preprint</span> <span data-semantic-scholar-id=""></span></p>
            <p class="mt-0 mb-0 small text-muted">We devise a hierarchical framework, AutoPal, that enables controllable and authentic adjustments to the agent's persona based on user interactions. A persona-matching dataset is constructed to facilitate the learning of optimal persona adaptations. </p>
            
            <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                
                
                <a target="_blank" href="https://arxiv.org/abs/2406.13960">[Paper]</a>
                
                
            </p>

        </div>
    </div>
</div>

<div class="row no-gutters d-md-none border-bottom border-gray   lazy" data-src="/academic-homepage/assets/images/covers/autopal.jpg">
    <div class="w-100  " style="background-color: rgba(255,255,255,0.9);">
        <div class="d-flex align-items-start flex-column py-3 px-4">
            <div class="mb-auto"></div>
            <div>
                <h5 class="mt-0 mb-1 font-weight-normal">AutoPal: Autonomous Adaptation to Users for Personal AI Companionship</h5>
                <p class="mt-0 mb-0 small"><a class="text-body" target="_blank" href="https://yicheng98.github.io/">Yi Cheng</a>, <span class="text-body">
            Wenge Liu, </span><span class="text-body">
            <strong>Kaishuai Xu</strong>, </span><a class="text-body" target="_blank" href="https://wjhou.github.io/">Wenjun Hou</a>, <span class="text-body">
            Yi Ouyang, </span><a class="text-body" target="_blank" href="https://cooperleong00.github.io/">Chak Tou Leong</a>, <a class="text-body" target="_blank" href="https://www4.comp.polyu.edu.hk/~cswjli/">Wenjie Li</a>, <span class="text-body">
            Xian Wu, </span><a class="text-body" target="_blank" href="https://en.westlake.edu.cn/faculty/yefeng-zheng.html">Yefeng Zheng</a></p>
                <p class="mt-0 mb-0 small"><i>arXiv preprint</i> 2024  <span class="badge badge-pill badge-publication badge-secondary">Preprint</span> <span data-semantic-scholar-id=""></span></p>
                <p class="mt-0 mb-0 small text-muted">We devise a hierarchical framework, AutoPal, that enables controllable and authentic adjustments to the agent's persona based on user interactions. A persona-matching dataset is constructed to facilitate the learning of optimal persona adaptations. </p>
                
                <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                    
                    
                    <a target="_blank" href="https://arxiv.org/abs/2406.13960">[Paper]</a>
                    
                    
                </p>
            </div>
        </div>
    </div>

</div>
            
                
<div class="d-none d-md-block">
    <div class="row no-gutters border-bottom border-gray">
        <div class="col-md-3 col-xl-2 mb-md-0 p-md-3"><img data-src="/academic-homepage/assets/images/covers/finetuning-attacks.jpg" alt="No Two Devils Alike: Unveiling Distinct Mechanisms of Fine-tuning Attacks" class="lazy w-100 rounded-sm" src="/academic-homepage/assets/images/empty_300x200.png"></div>
        <div class="col-md-9 col-xl-10 p-3 pl-md-0">
            <h5 class="mt-0 mb-1 font-weight-normal">No Two Devils Alike: Unveiling Distinct Mechanisms of Fine-tuning Attacks</h5>
            <p class="mt-0 mb-0 small"><a class="text-body" target="_blank" href="https://cooperleong00.github.io/">Chak Tou Leong</a>, <a class="text-body" target="_blank" href="https://yicheng98.github.io/">Yi Cheng</a>, <span class="text-body">
            <strong>Kaishuai Xu</strong>, </span><a class="text-body" target="_blank" href="https://iwangjian.github.io/">Jian Wang</a>, <span class="text-body">
            Hanlin Wang, </span><a class="text-body" target="_blank" href="https://www4.comp.polyu.edu.hk/~cswjli/">Wenjie Li</a></p>
            <p class="mt-0 mb-0 small"><i>arXiv preprint</i> 2024  <span class="badge badge-pill badge-publication badge-secondary">Preprint</span> <span data-semantic-scholar-id=""></span></p>
            <p class="mt-0 mb-0 small text-muted">We break down the safeguarding process of an LLM when encountered with harmful instructions into three stages: (1) recognizing harmful instructions, (2) generating an initial refusing tone, and (3) completing the refusal response. Accordingly, we investigate whether and how different attack strategies could influence each stage of this safeguarding process. </p>
            
            <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                
                
                <a target="_blank" href="https://arxiv.org/abs/2405.16229">[Paper]</a>
                
                
            </p>

        </div>
    </div>
</div>

<div class="row no-gutters d-md-none border-bottom border-gray   lazy" data-src="/academic-homepage/assets/images/covers/finetuning-attacks.jpg">
    <div class="w-100  " style="background-color: rgba(255,255,255,0.9);">
        <div class="d-flex align-items-start flex-column py-3 px-4">
            <div class="mb-auto"></div>
            <div>
                <h5 class="mt-0 mb-1 font-weight-normal">No Two Devils Alike: Unveiling Distinct Mechanisms of Fine-tuning Attacks</h5>
                <p class="mt-0 mb-0 small"><a class="text-body" target="_blank" href="https://cooperleong00.github.io/">Chak Tou Leong</a>, <a class="text-body" target="_blank" href="https://yicheng98.github.io/">Yi Cheng</a>, <span class="text-body">
            <strong>Kaishuai Xu</strong>, </span><a class="text-body" target="_blank" href="https://iwangjian.github.io/">Jian Wang</a>, <span class="text-body">
            Hanlin Wang, </span><a class="text-body" target="_blank" href="https://www4.comp.polyu.edu.hk/~cswjli/">Wenjie Li</a></p>
                <p class="mt-0 mb-0 small"><i>arXiv preprint</i> 2024  <span class="badge badge-pill badge-publication badge-secondary">Preprint</span> <span data-semantic-scholar-id=""></span></p>
                <p class="mt-0 mb-0 small text-muted">We break down the safeguarding process of an LLM when encountered with harmful instructions into three stages: (1) recognizing harmful instructions, (2) generating an initial refusing tone, and (3) completing the refusal response. Accordingly, we investigate whether and how different attack strategies could influence each stage of this safeguarding process. </p>
                
                <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                    
                    
                    <a target="_blank" href="https://arxiv.org/abs/2405.16229">[Paper]</a>
                    
                    
                </p>
            </div>
        </div>
    </div>

</div>
            
                
<div class="d-none d-md-block">
    <div class="row no-gutters  border-gray">
        <div class="col-md-3 col-xl-2 mb-md-0 p-md-3"><img data-src="/academic-homepage/assets/images/covers/iaddx.jpg" alt="Medical Dialogue Generation via Intuitive-then-Analytical Differential Diagnosis" class="lazy w-100 rounded-sm" src="/academic-homepage/assets/images/empty_300x200.png"></div>
        <div class="col-md-9 col-xl-10 p-3 pl-md-0">
            <h5 class="mt-0 mb-1 font-weight-normal">Medical Dialogue Generation via Intuitive-then-Analytical Differential Diagnosis</h5>
            <p class="mt-0 mb-0 small"><span class="text-body">
            <strong>Kaishuai Xu</strong>, </span><a class="text-body" target="_blank" href="https://wjhou.github.io/">Wenjun Hou</a>, <a class="text-body" target="_blank" href="https://yicheng98.github.io/">Yi Cheng</a>, <a class="text-body" target="_blank" href="https://iwangjian.github.io/">Jian Wang</a>, <a class="text-body" target="_blank" href="https://www4.comp.polyu.edu.hk/~cswjli/">Wenjie Li</a></p>
            <p class="mt-0 mb-0 small"><i>arXiv preprint</i> 2024  <span class="badge badge-pill badge-publication badge-secondary">Preprint</span> <span data-semantic-scholar-id=""></span></p>
            <p class="mt-0 mb-0 small text-muted">We propose a medical dialogue generation framework with the Intuitive-then-Analytic Differential Diagnosis (IADDx). Our method starts with a differential diagnosis via retrieval-based intuitive association and subsequently refines it through a graph-enhanced analytic procedure. The resulting differential diagnosis is then used to retrieve medical knowledge and guide response generation.</p>
            
            <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                
                
                <a target="_blank" href="https://arxiv.org/abs/2401.06541">[Paper]</a>
                
                
            </p>

        </div>
    </div>
</div>

<div class="row no-gutters d-md-none  border-gray  rounded-xl-bottom lazy" data-src="/academic-homepage/assets/images/covers/iaddx.jpg">
    <div class="w-100  rounded-xl-bottom" style="background-color: rgba(255,255,255,0.9);">
        <div class="d-flex align-items-start flex-column py-3 px-4">
            <div class="mb-auto"></div>
            <div>
                <h5 class="mt-0 mb-1 font-weight-normal">Medical Dialogue Generation via Intuitive-then-Analytical Differential Diagnosis</h5>
                <p class="mt-0 mb-0 small"><span class="text-body">
            <strong>Kaishuai Xu</strong>, </span><a class="text-body" target="_blank" href="https://wjhou.github.io/">Wenjun Hou</a>, <a class="text-body" target="_blank" href="https://yicheng98.github.io/">Yi Cheng</a>, <a class="text-body" target="_blank" href="https://iwangjian.github.io/">Jian Wang</a>, <a class="text-body" target="_blank" href="https://www4.comp.polyu.edu.hk/~cswjli/">Wenjie Li</a></p>
                <p class="mt-0 mb-0 small"><i>arXiv preprint</i> 2024  <span class="badge badge-pill badge-publication badge-secondary">Preprint</span> <span data-semantic-scholar-id=""></span></p>
                <p class="mt-0 mb-0 small text-muted">We propose a medical dialogue generation framework with the Intuitive-then-Analytic Differential Diagnosis (IADDx). Our method starts with a differential diagnosis via retrieval-based intuitive association and subsequently refines it through a graph-enhanced analytic procedure. The resulting differential diagnosis is then used to retrieve medical knowledge and guide response generation.</p>
                
                <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                    
                    
                    <a target="_blank" href="https://arxiv.org/abs/2401.06541">[Paper]</a>
                    
                    
                </p>
            </div>
        </div>
    </div>

</div>
            
        </div>
        
        
        <h2 class="pt-4" id="year-2023">2023</h2>
        <div class="my-0 p-0 bg-white shadow-sm rounded-xl">
            
                
<div class="d-none d-md-block">
    <div class="row no-gutters border-bottom border-gray">
        <div class="col-md-3 col-xl-2 mb-md-0 p-md-3"><img data-src="/academic-homepage/assets/images/covers/recap.jpg" alt="RECAP: Towards Precise Radiology Report Generation via Dynamic Disease Progression Reasoning" class="lazy w-100 rounded-sm" src="/academic-homepage/assets/images/empty_300x200.png"></div>
        <div class="col-md-9 col-xl-10 p-3 pl-md-0">
            <h5 class="mt-0 mb-1 font-weight-normal">RECAP: Towards Precise Radiology Report Generation via Dynamic Disease Progression Reasoning</h5>
            <p class="mt-0 mb-0 small"><a class="text-body" target="_blank" href="https://wjhou.github.io/">Wenjun Hou</a>, <a class="text-body" target="_blank" href="https://yicheng98.github.io/">Yi Cheng*</a>, <span class="text-body">
            <strong>Kaishuai Xu*</strong>, </span><a class="text-body" target="_blank" href="https://www4.comp.polyu.edu.hk/~cswjli/">Wenjie Li</a>, <a class="text-body" target="_blank" href="https://www.sustech.edu.cn/en/faculties/liujiang.html">Jiang Liu</a>
<mark>(* <i> equal contribution</i>)</mark></p>
            <p class="mt-0 mb-0 small"><i>Findings of the Association for Computational Linguistics: EMNLP 2023</i>   <span class="badge badge-pill badge-publication badge-warning">Findings</span> <span data-semantic-scholar-id=""></span></p>
            <p class="mt-0 mb-0 small text-muted">We propose RECAP, which generates precise and accurate radiology reports via dynamic disease progression reasoning. Specifically, RECAP first predicts the observations and progressions (i.e., spatiotemporal information) given two consecutive radiographs. It then combines the historical records, spatiotemporal information, and radiographs for report generation, where a disease progression graph and dynamic progression reasoning mechanism are devised to accurately select the attributes of each observation and progression. Extensive experiments on two publicly available datasets demonstrate the effectiveness of our model.</p>
            
            <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                
                
                <a target="_blank" href="https://aclanthology.org/2023.findings-emnlp.140/">[Paper]</a>
                
                
                
                <a target="_blank" href="https://github.com/wjhou/Recap">[Code]</a>
                
                
            </p>

        </div>
    </div>
</div>

<div class="row no-gutters d-md-none border-bottom border-gray rounded-xl-top  lazy" data-src="/academic-homepage/assets/images/covers/recap.jpg">
    <div class="w-100 rounded-xl-top " style="background-color: rgba(255,255,255,0.9);">
        <div class="d-flex align-items-start flex-column py-3 px-4">
            <div class="mb-auto"></div>
            <div>
                <h5 class="mt-0 mb-1 font-weight-normal">RECAP: Towards Precise Radiology Report Generation via Dynamic Disease Progression Reasoning</h5>
                <p class="mt-0 mb-0 small"><a class="text-body" target="_blank" href="https://wjhou.github.io/">Wenjun Hou</a>, <a class="text-body" target="_blank" href="https://yicheng98.github.io/">Yi Cheng*</a>, <span class="text-body">
            <strong>Kaishuai Xu*</strong>, </span><a class="text-body" target="_blank" href="https://www4.comp.polyu.edu.hk/~cswjli/">Wenjie Li</a>, <a class="text-body" target="_blank" href="https://www.sustech.edu.cn/en/faculties/liujiang.html">Jiang Liu</a>
<mark>(* <i> equal contribution</i>)</mark></p>
                <p class="mt-0 mb-0 small"><i>Findings of the Association for Computational Linguistics: EMNLP 2023</i>   <span class="badge badge-pill badge-publication badge-warning">Findings</span> <span data-semantic-scholar-id=""></span></p>
                <p class="mt-0 mb-0 small text-muted">We propose RECAP, which generates precise and accurate radiology reports via dynamic disease progression reasoning. Specifically, RECAP first predicts the observations and progressions (i.e., spatiotemporal information) given two consecutive radiographs. It then combines the historical records, spatiotemporal information, and radiographs for report generation, where a disease progression graph and dynamic progression reasoning mechanism are devised to accurately select the attributes of each observation and progression. Extensive experiments on two publicly available datasets demonstrate the effectiveness of our model.</p>
                
                <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                    
                    
                    <a target="_blank" href="https://aclanthology.org/2023.findings-emnlp.140/">[Paper]</a>
                    
                    
                    
                    <a target="_blank" href="https://github.com/wjhou/Recap">[Code]</a>
                    
                    
                </p>
            </div>
        </div>
    </div>

</div>
            
                
<div class="d-none d-md-block">
    <div class="row no-gutters border-bottom border-gray">
        <div class="col-md-3 col-xl-2 mb-md-0 p-md-3"><img data-src="/academic-homepage/assets/images/covers/dfmed.jpg" alt="Medical Dialogue Generation via Dual Flow Modeling" class="lazy w-100 rounded-sm" src="/academic-homepage/assets/images/empty_300x200.png"></div>
        <div class="col-md-9 col-xl-10 p-3 pl-md-0">
            <h5 class="mt-0 mb-1 font-weight-normal">Medical Dialogue Generation via Dual Flow Modeling</h5>
            <p class="mt-0 mb-0 small"><span class="text-body">
            <strong>Kaishuai Xu</strong>, </span><a class="text-body" target="_blank" href="https://wjhou.github.io/">Wenjun Hou*</a>, <a class="text-body" target="_blank" href="https://yicheng98.github.io/">Yi Cheng*</a>, <a class="text-body" target="_blank" href="https://iwangjian.github.io/">Jian Wang</a>, <a class="text-body" target="_blank" href="https://www4.comp.polyu.edu.hk/~cswjli/">Wenjie Li</a>
<mark>(* <i> equal contribution</i>)</mark></p>
            <p class="mt-0 mb-0 small"><i>Findings of the Association for Computational Linguistics: ACL 2023</i>   <span class="badge badge-pill badge-publication badge-warning">Findings</span> <span data-semantic-scholar-id=""></span></p>
            <p class="mt-0 mb-0 small text-muted">We propose a Dual Flow enhanced Medical (DFMed) dialogue generation framework. It extracts the medical entities and dialogue acts used in the dialogue history and models their transitions with an entity-centric graph flow and a sequential act flow, respectively. We employ two sequential models to encode them and devise an interweaving component to enhance their interactions. Experiments on two datasets demonstrate that our method exceeds baselines in both automatic and manual evaluations.</p>
            
            <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                
                
                <a target="_blank" href="https://aclanthology.org/2023.findings-acl.423/">[Paper]</a>
                
                
                
                <a target="_blank" href="https://github.com/kaishxu/DFMed">[Code]</a>
                
                
            </p>

        </div>
    </div>
</div>

<div class="row no-gutters d-md-none border-bottom border-gray   lazy" data-src="/academic-homepage/assets/images/covers/dfmed.jpg">
    <div class="w-100  " style="background-color: rgba(255,255,255,0.9);">
        <div class="d-flex align-items-start flex-column py-3 px-4">
            <div class="mb-auto"></div>
            <div>
                <h5 class="mt-0 mb-1 font-weight-normal">Medical Dialogue Generation via Dual Flow Modeling</h5>
                <p class="mt-0 mb-0 small"><span class="text-body">
            <strong>Kaishuai Xu</strong>, </span><a class="text-body" target="_blank" href="https://wjhou.github.io/">Wenjun Hou*</a>, <a class="text-body" target="_blank" href="https://yicheng98.github.io/">Yi Cheng*</a>, <a class="text-body" target="_blank" href="https://iwangjian.github.io/">Jian Wang</a>, <a class="text-body" target="_blank" href="https://www4.comp.polyu.edu.hk/~cswjli/">Wenjie Li</a>
<mark>(* <i> equal contribution</i>)</mark></p>
                <p class="mt-0 mb-0 small"><i>Findings of the Association for Computational Linguistics: ACL 2023</i>   <span class="badge badge-pill badge-publication badge-warning">Findings</span> <span data-semantic-scholar-id=""></span></p>
                <p class="mt-0 mb-0 small text-muted">We propose a Dual Flow enhanced Medical (DFMed) dialogue generation framework. It extracts the medical entities and dialogue acts used in the dialogue history and models their transitions with an entity-centric graph flow and a sequential act flow, respectively. We employ two sequential models to encode them and devise an interweaving component to enhance their interactions. Experiments on two datasets demonstrate that our method exceeds baselines in both automatic and manual evaluations.</p>
                
                <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                    
                    
                    <a target="_blank" href="https://aclanthology.org/2023.findings-acl.423/">[Paper]</a>
                    
                    
                    
                    <a target="_blank" href="https://github.com/kaishxu/DFMed">[Code]</a>
                    
                    
                </p>
            </div>
        </div>
    </div>

</div>
            
                
<div class="d-none d-md-block">
    <div class="row no-gutters  border-gray">
        <div class="col-md-3 col-xl-2 mb-md-0 p-md-3"><img data-src="/academic-homepage/assets/images/covers/organ.jpg" alt="ORGAN: Observation-Guided Radiology Report Generation via Tree Reasoning" class="lazy w-100 rounded-sm" src="/academic-homepage/assets/images/empty_300x200.png"></div>
        <div class="col-md-9 col-xl-10 p-3 pl-md-0">
            <h5 class="mt-0 mb-1 font-weight-normal">ORGAN: Observation-Guided Radiology Report Generation via Tree Reasoning</h5>
            <p class="mt-0 mb-0 small"><a class="text-body" target="_blank" href="https://wjhou.github.io/">Wenjun Hou</a>, <span class="text-body">
            <strong>Kaishuai Xu*</strong>, </span><a class="text-body" target="_blank" href="https://yicheng98.github.io/">Yi Cheng*</a>, <a class="text-body" target="_blank" href="https://www4.comp.polyu.edu.hk/~cswjli/">Wenjie Li</a>, <a class="text-body" target="_blank" href="https://www.sustech.edu.cn/en/faculties/liujiang.html">Jiang Liu</a>
<mark>(* <i> equal contribution</i>)</mark></p>
            <p class="mt-0 mb-0 small"><i>Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL)</i> 2023  <span class="badge badge-pill badge-publication badge-primary">Main Proceedings</span> <span data-semantic-scholar-id=""></span></p>
            <p class="mt-0 mb-0 small text-muted">In this paper, we propose an Observation-guided radiology Report Generation framework (ORGan). It first produces an observation plan and then feeds both the plan and radiographs for report generation, where an observation graph and a tree reasoning mechanism are adopted to precisely enrich the plan information by capturing the multi-formats of each observation. Experimental results demonstrate that our framework outperforms previous state-of-the-art methods regarding text quality and clinical efficacy.</p>
            
            <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                
                
                <a target="_blank" href="https://aclanthology.org/2023.acl-long.451/">[Paper]</a>
                
                
                
                <a target="_blank" href="https://github.com/wjhou/ORGan">[Code]</a>
                
                
            </p>

        </div>
    </div>
</div>

<div class="row no-gutters d-md-none  border-gray  rounded-xl-bottom lazy" data-src="/academic-homepage/assets/images/covers/organ.jpg">
    <div class="w-100  rounded-xl-bottom" style="background-color: rgba(255,255,255,0.9);">
        <div class="d-flex align-items-start flex-column py-3 px-4">
            <div class="mb-auto"></div>
            <div>
                <h5 class="mt-0 mb-1 font-weight-normal">ORGAN: Observation-Guided Radiology Report Generation via Tree Reasoning</h5>
                <p class="mt-0 mb-0 small"><a class="text-body" target="_blank" href="https://wjhou.github.io/">Wenjun Hou</a>, <span class="text-body">
            <strong>Kaishuai Xu*</strong>, </span><a class="text-body" target="_blank" href="https://yicheng98.github.io/">Yi Cheng*</a>, <a class="text-body" target="_blank" href="https://www4.comp.polyu.edu.hk/~cswjli/">Wenjie Li</a>, <a class="text-body" target="_blank" href="https://www.sustech.edu.cn/en/faculties/liujiang.html">Jiang Liu</a>
<mark>(* <i> equal contribution</i>)</mark></p>
                <p class="mt-0 mb-0 small"><i>Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL)</i> 2023  <span class="badge badge-pill badge-publication badge-primary">Main Proceedings</span> <span data-semantic-scholar-id=""></span></p>
                <p class="mt-0 mb-0 small text-muted">In this paper, we propose an Observation-guided radiology Report Generation framework (ORGan). It first produces an observation plan and then feeds both the plan and radiographs for report generation, where an observation graph and a tree reasoning mechanism are adopted to precisely enrich the plan information by capturing the multi-formats of each observation. Experimental results demonstrate that our framework outperforms previous state-of-the-art methods regarding text quality and clinical efficacy.</p>
                
                <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                    
                    
                    <a target="_blank" href="https://aclanthology.org/2023.acl-long.451/">[Paper]</a>
                    
                    
                    
                    <a target="_blank" href="https://github.com/wjhou/ORGan">[Code]</a>
                    
                    
                </p>
            </div>
        </div>
    </div>

</div>
            
        </div>
        
    </div>

    <div class="col-2 d-none d-lg-block">
        <div id="navbar-year" class="nav nav-pills flex-column sticky-top" style="top: 80px">
            
            <a class="nav-link d-block" href="#year-2025">2025</a>
            
            <a class="nav-link d-block" href="#year-2024">2024</a>
            
            <a class="nav-link d-block" href="#year-2023">2023</a>
            
        </div>
    </div>

</div>

    </div>
    <footer class="footer">
    <div class="container-lg">
        <div class="row my-3">
            <div class="col-6">
                <div class="text-muted">
                    <i>Last updated: Sep 2025</i>
                </div>
            </div>
            <div class="col-6">
                <div class="text-right text-muted">
                    
                </div>
            </div>
        </div>
    </div>
</footer>


    <!-- Scripts -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/jquery.lazy/1.7.9/jquery.lazy.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.6.0/js/bootstrap.min.js" integrity="sha512-XKa9Hemdy1Ui3KSGgJdgMyYlUg1gM+QhL6cnlyTe2qzMCYm4nAZ1PsVerQzTTXzonUR+dmswHqgJPuwCq1MaAg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/github-buttons/2.14.2/buttons.min.js" integrity="sha512-OYwZx04hKFeFNYrWxIyo3atgGpb+cxU0ENWBZs72X7T9U+NoHPM1ftUn/Mfw7dRDXrqWA6M1wBg6z6fGE32aeA==" crossorigin="anonymous"></script>
    <script src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
    <script src="https://unpkg.com/imagesloaded@5/imagesloaded.pkgd.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js" integrity="sha384-7zkQWkzuo3B5mTepMUcHkMB5jZaolc2xDwL6VFqjFALcbeS9Ggm/Yr2r3Dy4lfFg" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js" integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
              delimiters: [
                  {left: '$$', right: '$$', display: true},
                  {left: '$', right: '$', display: false}
              ],
              throwOnError : false
            });
        });
    </script>
    <script src="/academic-homepage/assets/js/common.js"></script>
    <script src="/academic-homepage/assets/js/bubble_visual_hash.js"></script>
    <script src="/academic-homepage/assets/js/semantic_scholar_citation_count.js"></script>
</body>
</html>
